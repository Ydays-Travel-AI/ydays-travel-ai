{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# ======= Étape 1 : Chargement et nettoyage des données ======= #\n",
    "# Charger les données\n",
    "data = pd.read_csv(\"final_output.csv\")\n",
    "\n",
    "# Nettoyer les colonnes inutiles\n",
    "colonnes_a_supprimer = ['Latitude', 'Longitude', 'Periodes_regroupees', 'Covid19_mesures_specifiques', \n",
    "                        'Createur_de_la_donnee', 'SIT_diffuseur', 'Date_de_mise_a_jour', 'URI_ID_du_POI']\n",
    "data = data.drop(columns=colonnes_a_supprimer, errors='ignore')\n",
    "\n",
    "# Supprimer les lignes avec valeurs manquantes\n",
    "data = data.dropna(subset=['Nom_du_POI', 'Description', 'Code_postal_et_commune'])\n",
    "\n",
    "# Fonction pour nettoyer le texte\n",
    "def nettoyer_texte(texte):\n",
    "    texte = unicodedata.normalize('NFKD', str(texte))  # Enlever accents\n",
    "    texte = re.sub(r'[^a-zA-Z0-9\\s.,-]', '', texte)   # Garder lettres, chiffres, espace, ., , et -\n",
    "    texte = re.sub(r'\\s+', ' ', texte)  # Remplacer les espaces multiples\n",
    "    return texte.strip()\n",
    "\n",
    "data['Nom_du_POI'] = data['Nom_du_POI'].apply(nettoyer_texte)\n",
    "data['Description'] = data['Description'].apply(nettoyer_texte)\n",
    "\n",
    "# Supprimer les doublons\n",
    "data = data.drop_duplicates(subset=['Nom_du_POI', 'Code_postal_et_commune', 'Description'])\n",
    "\n",
    "# Créer des prompts structurés\n",
    "def transformer_donnees(row):\n",
    "    return f\"Destination : {row['Code_postal_et_commune'].split('#')[-1]}\\n\" \\\n",
    "           f\"Activité : {row['Nom_du_POI']}\\n\" \\\n",
    "           f\"Catégorie : {row['Categories_de_POI'].split('#')[-1]}\\n\" \\\n",
    "           f\"Description : {row['Description']}\\n\"\n",
    "\n",
    "data[\"prompt\"] = data.apply(transformer_donnees, axis=1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Étape 2 : Préparer les données pour GPT-2 ======= #\n",
    "dataset = Dataset.from_dict({\"text\": data[\"prompt\"].tolist()})\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ajoute un token de fin\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2LMHeadModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ======= Étape 3 : Configuration de l'entraînement ======= #\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2LMHeadModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForLanguageModeling(tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, mlm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GPT2LMHeadModel' is not defined"
     ]
    }
   ],
   "source": [
    "# ======= Étape 3 : Configuration de l'entraînement ======= #\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# ======= Étape 4 : Entraînement ======= #\n",
    "print(\"Début de l'entraînement...\")\n",
    "trainer.train()\n",
    "\n",
    "# ======= Étape 5 : Génération de texte ======= #\n",
    "print(\"\\nTest de génération de recommandations :\")\n",
    "prompt = \"Destination : Lyon\\nActivité recommandée :\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "resultat = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(resultat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "548137    Destination : Vendresse\\nActivité : Fete de la...\n",
       "Name: prompt, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "548137    Destination : Vendresse\\nActivité : Fete de la...\n",
       "Name: prompt, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
